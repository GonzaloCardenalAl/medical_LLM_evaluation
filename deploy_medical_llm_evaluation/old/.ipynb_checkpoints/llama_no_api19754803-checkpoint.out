Many modules are hidden in this stack. Use "module --show_hidden spider SOFTWARE" if you are not able to find the required software
# packages in environment at /cluster/home/gcardenal/miniconda3/envs/vllm:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                        main  
_openmp_mutex             5.1                       1_gnu  
accelerate                1.1.1                    pypi_0    pypi
aiohappyeyeballs          2.4.0                    pypi_0    pypi
aiohttp                   3.10.6                   pypi_0    pypi
aiosignal                 1.3.1                    pypi_0    pypi
annotated-types           0.7.0                    pypi_0    pypi
anthropic                 0.40.0                   pypi_0    pypi
anyio                     4.6.0                    pypi_0    pypi
argon2-cffi               23.1.0                   pypi_0    pypi
argon2-cffi-bindings      21.2.0                   pypi_0    pypi
arrow                     1.3.0                    pypi_0    pypi
asttokens                 3.0.0                    pypi_0    pypi
async-lru                 2.0.4                    pypi_0    pypi
async-timeout             4.0.3                    pypi_0    pypi
attrs                     24.2.0                   pypi_0    pypi
babel                     2.16.0                   pypi_0    pypi
beautifulsoup4            4.12.3                   pypi_0    pypi
bleach                    6.2.0                    pypi_0    pypi
bzip2                     1.0.8                h5eee18b_6  
ca-certificates           2024.7.2             h06a4308_0  
certifi                   2024.8.30                pypi_0    pypi
cffi                      1.17.1                   pypi_0    pypi
charset-normalizer        3.3.2                    pypi_0    pypi
click                     8.1.7                    pypi_0    pypi
cloudpickle               3.0.0                    pypi_0    pypi
comm                      0.2.2                    pypi_0    pypi
datasets                  3.0.1                    pypi_0    pypi
debugpy                   1.8.9                    pypi_0    pypi
decorator                 5.1.1                    pypi_0    pypi
defusedxml                0.7.1                    pypi_0    pypi
dill                      0.3.8                    pypi_0    pypi
diskcache                 5.6.3                    pypi_0    pypi
distro                    1.9.0                    pypi_0    pypi
einops                    0.8.0                    pypi_0    pypi
emoji                     2.14.0                   pypi_0    pypi
exceptiongroup            1.2.2                    pypi_0    pypi
executing                 2.1.0                    pypi_0    pypi
fastapi                   0.115.0                  pypi_0    pypi
fastjsonschema            2.21.1                   pypi_0    pypi
filelock                  3.16.1                   pypi_0    pypi
fqdn                      1.5.1                    pypi_0    pypi
frozenlist                1.4.1                    pypi_0    pypi
fsspec                    2024.6.1                 pypi_0    pypi
gguf                      0.10.0                   pypi_0    pypi
h11                       0.14.0                   pypi_0    pypi
httpcore                  1.0.5                    pypi_0    pypi
httptools                 0.6.1                    pypi_0    pypi
httpx                     0.28.0                   pypi_0    pypi
huggingface-hub           0.25.1                   pypi_0    pypi
idna                      3.10                     pypi_0    pypi
importlib-metadata        8.5.0                    pypi_0    pypi
interegular               0.3.3                    pypi_0    pypi
ipykernel                 6.29.5                   pypi_0    pypi
ipython                   8.30.0                   pypi_0    pypi
ipywidgets                8.1.5                    pypi_0    pypi
isoduration               20.11.0                  pypi_0    pypi
jedi                      0.19.2                   pypi_0    pypi
jinja2                    3.1.4                    pypi_0    pypi
jiter                     0.5.0                    pypi_0    pypi
json5                     0.10.0                   pypi_0    pypi
jsonpointer               3.0.0                    pypi_0    pypi
jsonschema                4.23.0                   pypi_0    pypi
jsonschema-specifications 2023.12.1                pypi_0    pypi
jupyter                   1.1.1                    pypi_0    pypi
jupyter-client            8.6.3                    pypi_0    pypi
jupyter-console           6.6.3                    pypi_0    pypi
jupyter-core              5.7.2                    pypi_0    pypi
jupyter-events            0.10.0                   pypi_0    pypi
jupyter-lsp               2.2.5                    pypi_0    pypi
jupyter-server            2.14.2                   pypi_0    pypi
jupyter-server-terminals  0.5.3                    pypi_0    pypi
jupyterlab                4.3.2                    pypi_0    pypi
jupyterlab-pygments       0.3.0                    pypi_0    pypi
jupyterlab-server         2.27.3                   pypi_0    pypi
jupyterlab-widgets        3.0.13                   pypi_0    pypi
lark                      1.2.2                    pypi_0    pypi
ld_impl_linux-64          2.40                 h12ee557_0  
libffi                    3.4.4                h6a678d5_1  
libgcc-ng                 11.2.0               h1234567_1  
libgomp                   11.2.0               h1234567_1  
libstdcxx-ng              11.2.0               h1234567_1  
libuuid                   1.41.5               h5eee18b_0  
llvmlite                  0.43.0                   pypi_0    pypi
lm-format-enforcer        0.10.6                   pypi_0    pypi
markupsafe                2.1.5                    pypi_0    pypi
matplotlib-inline         0.1.7                    pypi_0    pypi
mistral-common            1.4.3                    pypi_0    pypi
mistune                   3.0.2                    pypi_0    pypi
mpmath                    1.3.0                    pypi_0    pypi
msgpack                   1.1.0                    pypi_0    pypi
msgspec                   0.18.6                   pypi_0    pypi
multidict                 6.1.0                    pypi_0    pypi
multiprocess              0.70.16                  pypi_0    pypi
nbclient                  0.10.1                   pypi_0    pypi
nbconvert                 7.16.4                   pypi_0    pypi
nbformat                  5.10.4                   pypi_0    pypi
ncurses                   6.4                  h6a678d5_0  
nest-asyncio              1.6.0                    pypi_0    pypi
networkx                  3.3                      pypi_0    pypi
notebook                  7.3.1                    pypi_0    pypi
notebook-shim             0.2.4                    pypi_0    pypi
numba                     0.60.0                   pypi_0    pypi
numpy                     1.26.4                   pypi_0    pypi
nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi
nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi
nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi
nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi
nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi
nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi
nvidia-curand-cu12        10.3.2.106               pypi_0    pypi
nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi
nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi
nvidia-ml-py              12.560.30                pypi_0    pypi
nvidia-nccl-cu12          2.20.5                   pypi_0    pypi
nvidia-nvjitlink-cu12     12.6.68                  pypi_0    pypi
nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi
openai                    1.48.0                   pypi_0    pypi
openssl                   3.0.15               h5eee18b_0  
outlines                  0.0.46                   pypi_0    pypi
overrides                 7.7.0                    pypi_0    pypi
packaging                 24.1                     pypi_0    pypi
pandas                    2.2.3                    pypi_0    pypi
pandocfilters             1.5.1                    pypi_0    pypi
parso                     0.8.4                    pypi_0    pypi
partial-json-parser       0.2.1.1.post4            pypi_0    pypi
pexpect                   4.9.0                    pypi_0    pypi
pillow                    10.4.0                   pypi_0    pypi
pip                       24.2            py310h06a4308_0  
platformdirs              4.3.6                    pypi_0    pypi
prometheus-client         0.21.0                   pypi_0    pypi
prometheus-fastapi-instrumentator 7.0.0                    pypi_0    pypi
prompt-toolkit            3.0.48                   pypi_0    pypi
protobuf                  5.28.2                   pypi_0    pypi
psutil                    6.0.0                    pypi_0    pypi
ptyprocess                0.7.0                    pypi_0    pypi
pure-eval                 0.2.3                    pypi_0    pypi
py-cpuinfo                9.0.0                    pypi_0    pypi
pyairports                2.1.1                    pypi_0    pypi
pyarrow                   17.0.0                   pypi_0    pypi
pycountry                 24.6.1                   pypi_0    pypi
pycparser                 2.22                     pypi_0    pypi
pydantic                  2.9.2                    pypi_0    pypi
pydantic-core             2.23.4                   pypi_0    pypi
pygments                  2.18.0                   pypi_0    pypi
python                    3.10.14              h955ad1f_1  
python-dateutil           2.9.0.post0              pypi_0    pypi
python-dotenv             1.0.1                    pypi_0    pypi
python-json-logger        2.0.7                    pypi_0    pypi
pytz                      2024.2                   pypi_0    pypi
pyyaml                    6.0.2                    pypi_0    pypi
pyzmq                     26.2.0                   pypi_0    pypi
ray                       2.37.0                   pypi_0    pypi
readline                  8.2                  h5eee18b_0  
referencing               0.35.1                   pypi_0    pypi
regex                     2024.9.11                pypi_0    pypi
requests                  2.32.3                   pypi_0    pypi
rfc3339-validator         0.1.4                    pypi_0    pypi
rfc3986-validator         0.1.1                    pypi_0    pypi
rpds-py                   0.20.0                   pypi_0    pypi
safetensors               0.4.5                    pypi_0    pypi
send2trash                1.8.3                    pypi_0    pypi
sentencepiece             0.2.0                    pypi_0    pypi
setuptools                75.1.0          py310h06a4308_0  
six                       1.16.0                   pypi_0    pypi
sniffio                   1.3.1                    pypi_0    pypi
soupsieve                 2.6                      pypi_0    pypi
sqlite                    3.45.3               h5eee18b_0  
stack-data                0.6.3                    pypi_0    pypi
stanza                    1.9.2                    pypi_0    pypi
starlette                 0.38.6                   pypi_0    pypi
sympy                     1.13.3                   pypi_0    pypi
terminado                 0.18.1                   pypi_0    pypi
tiktoken                  0.7.0                    pypi_0    pypi
timm                      1.0.12                   pypi_0    pypi
tinycss2                  1.4.0                    pypi_0    pypi
tk                        8.6.14               h39e8969_0  
tokenizers                0.20.0                   pypi_0    pypi
tomli                     2.2.1                    pypi_0    pypi
torch                     2.4.0                    pypi_0    pypi
torchvision               0.19.0                   pypi_0    pypi
tornado                   6.4.2                    pypi_0    pypi
tqdm                      4.66.5                   pypi_0    pypi
traitlets                 5.14.3                   pypi_0    pypi
transformers              4.45.0                   pypi_0    pypi
triton                    3.0.0                    pypi_0    pypi
types-python-dateutil     2.9.0.20241003           pypi_0    pypi
typing-extensions         4.12.2                   pypi_0    pypi
tzdata                    2024.2                   pypi_0    pypi
uri-template              1.3.0                    pypi_0    pypi
urllib3                   2.2.3                    pypi_0    pypi
uvicorn                   0.30.6                   pypi_0    pypi
uvloop                    0.20.0                   pypi_0    pypi
vllm                      0.6.2                    pypi_0    pypi
watchfiles                0.24.0                   pypi_0    pypi
wcwidth                   0.2.13                   pypi_0    pypi
webcolors                 24.11.1                  pypi_0    pypi
webencodings              0.5.1                    pypi_0    pypi
websocket-client          1.8.0                    pypi_0    pypi
websockets                13.1                     pypi_0    pypi
wheel                     0.44.0          py310h06a4308_0  
widgetsnbextension        4.0.13                   pypi_0    pypi
xformers                  0.0.27.post2             pypi_0    pypi
xxhash                    3.5.0                    pypi_0    pypi
xz                        5.4.6                h5eee18b_1  
yarl                      1.12.1                   pypi_0    pypi
zipp                      3.20.2                   pypi_0    pypi
zlib                      1.2.13               h5eee18b_1  
Package                           Version
--------------------------------- --------------
accelerate                        1.1.1
aiohappyeyeballs                  2.4.0
aiohttp                           3.10.6
aiosignal                         1.3.1
annotated-types                   0.7.0
anthropic                         0.40.0
anyio                             4.6.0
argon2-cffi                       23.1.0
argon2-cffi-bindings              21.2.0
arrow                             1.3.0
asttokens                         3.0.0
async-lru                         2.0.4
async-timeout                     4.0.3
attrs                             24.2.0
babel                             2.16.0
beautifulsoup4                    4.12.3
bleach                            6.2.0
certifi                           2024.8.30
cffi                              1.17.1
charset-normalizer                3.3.2
click                             8.1.7
cloudpickle                       3.0.0
comm                              0.2.2
datasets                          3.0.1
debugpy                           1.8.9
decorator                         5.1.1
defusedxml                        0.7.1
dill                              0.3.8
diskcache                         5.6.3
distro                            1.9.0
einops                            0.8.0
emoji                             2.14.0
exceptiongroup                    1.2.2
executing                         2.1.0
fastapi                           0.115.0
fastjsonschema                    2.21.1
filelock                          3.16.1
fqdn                              1.5.1
frozenlist                        1.4.1
fsspec                            2024.6.1
gguf                              0.10.0
h11                               0.14.0
httpcore                          1.0.5
httptools                         0.6.1
httpx                             0.28.0
huggingface-hub                   0.25.1
idna                              3.10
importlib_metadata                8.5.0
interegular                       0.3.3
ipykernel                         6.29.5
ipython                           8.30.0
ipywidgets                        8.1.5
isoduration                       20.11.0
jedi                              0.19.2
Jinja2                            3.1.4
jiter                             0.5.0
json5                             0.10.0
jsonpointer                       3.0.0
jsonschema                        4.23.0
jsonschema-specifications         2023.12.1
jupyter                           1.1.1
jupyter_client                    8.6.3
jupyter-console                   6.6.3
jupyter_core                      5.7.2
jupyter-events                    0.10.0
jupyter-lsp                       2.2.5
jupyter_server                    2.14.2
jupyter_server_terminals          0.5.3
jupyterlab                        4.3.2
jupyterlab_pygments               0.3.0
jupyterlab_server                 2.27.3
jupyterlab_widgets                3.0.13
lark                              1.2.2
llvmlite                          0.43.0
lm-format-enforcer                0.10.6
MarkupSafe                        2.1.5
matplotlib-inline                 0.1.7
mistral_common                    1.4.3
mistune                           3.0.2
mpmath                            1.3.0
msgpack                           1.1.0
msgspec                           0.18.6
multidict                         6.1.0
multiprocess                      0.70.16
nbclient                          0.10.1
nbconvert                         7.16.4
nbformat                          5.10.4
nest-asyncio                      1.6.0
networkx                          3.3
notebook                          7.3.1
notebook_shim                     0.2.4
numba                             0.60.0
numpy                             1.26.4
nvidia-cublas-cu12                12.1.3.1
nvidia-cuda-cupti-cu12            12.1.105
nvidia-cuda-nvrtc-cu12            12.1.105
nvidia-cuda-runtime-cu12          12.1.105
nvidia-cudnn-cu12                 9.1.0.70
nvidia-cufft-cu12                 11.0.2.54
nvidia-curand-cu12                10.3.2.106
nvidia-cusolver-cu12              11.4.5.107
nvidia-cusparse-cu12              12.1.0.106
nvidia-ml-py                      12.560.30
nvidia-nccl-cu12                  2.20.5
nvidia-nvjitlink-cu12             12.6.68
nvidia-nvtx-cu12                  12.1.105
openai                            1.48.0
outlines                          0.0.46
overrides                         7.7.0
packaging                         24.1
pandas                            2.2.3
pandocfilters                     1.5.1
parso                             0.8.4
partial-json-parser               0.2.1.1.post4
pexpect                           4.9.0
pillow                            10.4.0
pip                               24.2
platformdirs                      4.3.6
prometheus_client                 0.21.0
prometheus-fastapi-instrumentator 7.0.0
prompt_toolkit                    3.0.48
protobuf                          5.28.2
psutil                            6.0.0
ptyprocess                        0.7.0
pure_eval                         0.2.3
py-cpuinfo                        9.0.0
pyairports                        2.1.1
pyarrow                           17.0.0
pycountry                         24.6.1
pycparser                         2.22
pydantic                          2.9.2
pydantic_core                     2.23.4
Pygments                          2.18.0
python-dateutil                   2.9.0.post0
python-dotenv                     1.0.1
python-json-logger                2.0.7
pytz                              2024.2
PyYAML                            6.0.2
pyzmq                             26.2.0
ray                               2.37.0
referencing                       0.35.1
regex                             2024.9.11
requests                          2.32.3
rfc3339-validator                 0.1.4
rfc3986-validator                 0.1.1
rpds-py                           0.20.0
safetensors                       0.4.5
Send2Trash                        1.8.3
sentencepiece                     0.2.0
setuptools                        75.1.0
six                               1.16.0
sniffio                           1.3.1
soupsieve                         2.6
stack-data                        0.6.3
stanza                            1.9.2
starlette                         0.38.6
sympy                             1.13.3
terminado                         0.18.1
tiktoken                          0.7.0
timm                              1.0.12
tinycss2                          1.4.0
tokenizers                        0.20.0
tomli                             2.2.1
torch                             2.4.0
torchvision                       0.19.0
tornado                           6.4.2
tqdm                              4.66.5
traitlets                         5.14.3
transformers                      4.45.0
triton                            3.0.0
types-python-dateutil             2.9.0.20241003
typing_extensions                 4.12.2
tzdata                            2024.2
uri-template                      1.3.0
urllib3                           2.2.3
uvicorn                           0.30.6
uvloop                            0.20.0
vllm                              0.6.2
watchfiles                        0.24.0
wcwidth                           0.2.13
webcolors                         24.11.1
webencodings                      0.5.1
websocket-client                  1.8.0
websockets                        13.1
wheel                             0.44.0
widgetsnbextension                4.0.13
xformers                          0.0.27.post2
xxhash                            3.5.0
yarl                              1.12.1
zipp                              3.20.2

Currently Loaded Modules:
  1) eth_proxy       5) r/4.3.2         9) nccl/2.18.3-1
  2) julia/1.10.3    6) hdf5/1.14.3    10) openblas/0.3.24
  3) gcc/12.2.0      7) python/3.11.6  11) python_cuda/3.11.6
  4) stack/2024-06   8) cuda/12.1.1

 

Testing Slurm Variables...
SLURM_JOB_USER=gcardenal
SLURM_TASKS_PER_NODE=1
SLURM_JOB_UID=608619
SLURM_TASK_PID=1158608
SLURM_JOB_GPUS=2,5,6
SLURM_LOCALID=0
SLURM_SUBMIT_DIR=/cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation
SLURMD_NODENAME=eu-a65-05
SLURM_JOB_START_TIME=1735921029
SLURM_CLUSTER_NAME=euler-24-production
SLURM_JOB_END_TIME=1735935429
SLURM_CPUS_ON_NODE=8
SLURM_JOB_CPUS_PER_NODE=8
SLURM_GPUS_ON_NODE=3
SLURM_GTIDS=0
SLURM_JOB_PARTITION=gpupr.4h
SLURM_TRES_PER_TASK=cpu=8
SLURM_JOB_NUM_NODES=1
SLURM_JOBID=19754803
SLURM_GPUS=nvidia_a100_80gb_pcie:3
SLURM_JOB_QOS=es_ilic/gpupr/4
SLURM_PROCID=0
SLURM_CPUS_PER_TASK=8
SLURM_NTASKS=1
SLURM_TOPOLOGY_ADDR=.euler_a100_80_lca_ib.eu-a65-05
SLURM_TOPOLOGY_ADDR_PATTERN=switch.switch.node
SLURM_MEM_PER_CPU=10240
SLURM_SCRIPT_CONTEXT=prolog_task
SLURM_NODELIST=eu-a65-05
SLURM_JOB_ACCOUNT=gpupr/es_ilic
SLURM_PRIO_PROCESS=0
SLURM_NPROCS=1
SLURM_NNODES=1
SLURM_SUBMIT_HOST=eu-lo-g2-034
SLURM_JOB_ID=19754803
SLURM_NODEID=0
SLURM_CONF=/cluster/slurm/adm/etc/slurm.conf
SLURM_JOB_NAME=llama_no_api
SLURM_JOB_GID=492010
SLURM_JOB_NODELIST=eu-a65-05
Fri Jan  3 17:17:19 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:0B:00.0 Off |                    0 |
| N/A   34C    P0             45W /  300W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          On  |   00000000:88:00.0 Off |                    0 |
| N/A   36C    P0             45W /  300W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          On  |   00000000:8C:00.0 Off |                    0 |
| N/A   32C    P0             44W /  300W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Node IP: 10.205.9.14
JobId=19754803 JobName=llama_no_api
   UserId=gcardenal(608619) GroupId=gcardenal-group(492010) MCS_label=N/A
   Priority=5824 Nice=0 Account=gpupr/es_ilic QOS=es_ilic/gpupr/4
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:11 TimeLimit=04:00:00 TimeMin=N/A
   SubmitTime=2025-01-03T01:47:12 EligibleTime=2025-01-03T01:47:12
   AccrueTime=2025-01-03T01:47:12
   StartTime=2025-01-03T17:17:09 EndTime=2025-01-03T21:17:09 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-01-03T17:17:09 Scheduler=Backfill
   Partition=gpupr.4h AllocNode:Sid=eu-lo-g2-034:3116534
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=eu-a65-05
   BatchHost=eu-a65-05
   NumNodes=1 NumCPUs=8 NumTasks=1 CPUs/Task=8 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=8,mem=80G,node=1,billing=347340,gres/gpu=3,gres/gpu:nvidia_a100_80gb_pcie=3,gres/gpumem=85899345920
   AllocTRES=cpu=8,mem=80G,node=1,billing=347340,gres/gpu=3,gres/gpu:nvidia_a100_80gb_pcie=3,gres/gpumem=0
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:1 CoreSpec=*
   MinCPUsNode=8 MinMemoryCPU=10G MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/generate_answers.sh
   WorkDir=/cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation
   StdErr=/cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/llama_no_api19754803.out
   StdIn=/dev/null
   StdOut=/cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/llama_no_api19754803.out
   TresPerJob=gres/gpu:nvidia_a100_80gb_pcie:3
   TresPerNode=gres/gpumem:80G
   TresPerTask=cpu=8
   

Running in standalone mode...
I0103 17:17:23.025000 22518965430080 torch/distributed/run.py:859] 
I0103 17:17:23.025000 22518965430080 torch/distributed/run.py:859] **************************************
I0103 17:17:23.025000 22518965430080 torch/distributed/run.py:859] Rendezvous info:
I0103 17:17:23.025000 22518965430080 torch/distributed/run.py:859] --rdzv-backend=c10d --rdzv-endpoint=localhost:0 --rdzv-id=f1f643a3-7ad9-470c-9bd5-3996c0a0b945
I0103 17:17:23.025000 22518965430080 torch/distributed/run.py:859] **************************************
I0103 17:17:23.025000 22518965430080 torch/distributed/run.py:859] 
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188] Starting elastic_operator with launch configs:
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   entrypoint       : get_model_answers_and_prompt_generation.py
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   min_nodes        : 1
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   max_nodes        : 1
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   nproc_per_node   : 1
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   run_id           : f1f643a3-7ad9-470c-9bd5-3996c0a0b945
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   rdzv_backend     : c10d
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   rdzv_endpoint    : localhost:0
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   rdzv_configs     : {'timeout': 900}
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   max_restarts     : 0
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   monitor_interval : 5
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   log_dir          : /scratch/tmp.19754803.gcardenal/torchelastic_m6ryi0rd
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188]   metrics_cfg      : {}
I0103 17:17:23.025000 22518965430080 torch/distributed/launcher/api.py:188] 
I0103 17:17:23.037000 22518965430080 torch/distributed/elastic/agent/server/api.py:866] [default] starting workers for entrypoint: python3
I0103 17:17:23.037000 22518965430080 torch/distributed/elastic/agent/server/api.py:699] [default] Rendezvous'ing worker group
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568] [default] Rendezvous complete for workers. Result:
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568]   restart_count=0
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568]   master_addr=eu-a65-05.euler.ethz.ch
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568]   master_port=50573
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568]   group_rank=0
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568]   group_world_size=1
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568]   local_ranks=[0]
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568]   role_ranks=[0]
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568]   global_ranks=[0]
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568]   role_world_sizes=[1]
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568]   global_world_sizes=[1]
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:568] 
I0103 17:17:23.223000 22518965430080 torch/distributed/elastic/agent/server/api.py:707] [default] Starting worker group
I0103 17:17:23.224000 22518965430080 torch/distributed/elastic/agent/server/local_elastic_agent.py:168] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
I0103 17:17:23.224000 22518965430080 torch/distributed/elastic/multiprocessing/api.py:263] log directory set to: /scratch/tmp.19754803.gcardenal/torchelastic_m6ryi0rd/f1f643a3-7ad9-470c-9bd5-3996c0a0b945_gcowcn3y
I0103 17:17:23.225000 22518965430080 torch/distributed/elastic/multiprocessing/api.py:358] Setting worker0 reply file to: /scratch/tmp.19754803.gcardenal/torchelastic_m6ryi0rd/f1f643a3-7ad9-470c-9bd5-3996c0a0b945_gcowcn3y/attempt_0/0/error.json
API call for Llama failed with error: Connection error.
Falling back to local Llama inference - Loading model...
2025-01-03 17:17:32.882085: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-01-03 17:17:32.882133: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-01-03 17:17:32.883495: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-03 17:17:32.888912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-03 17:17:34.692616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Loading checkpoint shards:   0%|                    | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▍           | 1/30 [00:13<06:30, 13.48s/it]Loading checkpoint shards:   7%|▊           | 2/30 [00:20<04:33,  9.75s/it]Loading checkpoint shards:  10%|█▏          | 3/30 [00:27<03:51,  8.56s/it]Loading checkpoint shards:  13%|█▌          | 4/30 [00:37<03:55,  9.05s/it]Loading checkpoint shards:  17%|██          | 5/30 [00:52<04:43, 11.33s/it]Loading checkpoint shards:  20%|██▍         | 6/30 [01:07<05:01, 12.55s/it]Loading checkpoint shards:  23%|██▊         | 7/30 [01:16<04:17, 11.22s/it]Loading checkpoint shards:  27%|███▏        | 8/30 [01:27<04:05, 11.17s/it]Loading checkpoint shards:  30%|███▌        | 9/30 [01:35<03:32, 10.10s/it]Loading checkpoint shards:  33%|███▋       | 10/30 [01:41<02:57,  8.89s/it]Loading checkpoint shards:  37%|████       | 11/30 [01:58<03:37, 11.43s/it]Loading checkpoint shards:  40%|████▍      | 12/30 [02:07<03:14, 10.80s/it]Loading checkpoint shards:  43%|████▊      | 13/30 [02:28<03:54, 13.82s/it]Loading checkpoint shards:  47%|█████▏     | 14/30 [02:37<03:19, 12.45s/it]Loading checkpoint shards:  50%|█████▌     | 15/30 [02:50<03:06, 12.43s/it]Loading checkpoint shards:  53%|█████▊     | 16/30 [02:56<02:27, 10.51s/it]Loading checkpoint shards:  57%|██████▏    | 17/30 [03:02<02:00,  9.27s/it]Loading checkpoint shards:  60%|██████▌    | 18/30 [03:12<01:52,  9.35s/it]Loading checkpoint shards:  63%|██████▉    | 19/30 [03:21<01:42,  9.36s/it]Loading checkpoint shards:  67%|███████▎   | 20/30 [03:28<01:25,  8.54s/it]Loading checkpoint shards:  70%|███████▋   | 21/30 [03:34<01:09,  7.75s/it]Loading checkpoint shards:  73%|████████   | 22/30 [03:40<00:58,  7.33s/it]Loading checkpoint shards:  77%|████████▍  | 23/30 [03:49<00:53,  7.69s/it]Loading checkpoint shards:  80%|████████▊  | 24/30 [03:55<00:43,  7.26s/it]Loading checkpoint shards:  83%|█████████▏ | 25/30 [04:02<00:36,  7.24s/it]Loading checkpoint shards:  87%|█████████▌ | 26/30 [04:08<00:27,  6.93s/it]Loading checkpoint shards:  90%|█████████▉ | 27/30 [04:14<00:19,  6.66s/it]Loading checkpoint shards:  93%|██████████▎| 28/30 [04:21<00:13,  6.59s/it]Loading checkpoint shards:  97%|██████████▋| 29/30 [04:29<00:07,  7.17s/it]Loading checkpoint shards: 100%|███████████| 30/30 [04:32<00:00,  5.81s/it]Loading checkpoint shards: 100%|███████████| 30/30 [04:32<00:00,  9.08s/it]
Starting inference for question locally (Llama): How is HIV diagnosed?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What are the different stages of HIV?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How is HIV transmitted?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What comorbidities are common among people living 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How can HIV be prevented?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How frequently ART must be taken?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How can we prevent Perinatal transmission of HIV?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What are the main cell types infected with HIV?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is the difference between HIV and AIDS?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How is HIV not transmitted?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is known about the switch that occurs between
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=1, iteration=1) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_1.1_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How is HIV diagnosed?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What are the different stages of HIV?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How is HIV transmitted?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What comorbidities are common among people living 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How can HIV be prevented?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How frequently ART must be taken?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How can we prevent Perinatal transmission of HIV?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What are the main cell types infected with HIV?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is the difference between HIV and AIDS?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How is HIV not transmitted?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is known about the switch that occurs between
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=1, iteration=2) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_1.2_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How is HIV diagnosed?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What are the different stages of HIV?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How is HIV transmitted?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What comorbidities are common among people living 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How can HIV be prevented?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How frequently ART must be taken?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How can we prevent Perinatal transmission of HIV?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What are the main cell types infected with HIV?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is the difference between HIV and AIDS?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): How is HIV not transmitted?
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is known about the switch that occurs between
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=1, iteration=3) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_1.3_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Which five antiretroviral drugs were most frequent
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Which five antiretroviral drugs were most frequent
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Is there a difference in the overall birth rate in
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Is there a difference in the overall birth rate in
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is the most common source of HIV infection in
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): In Switzerland, based on self-reported treatment a
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): From 2020, what is the proportion of PLWH in Switz
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): From 2020, what proportion of PWH in Switzerland c
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): In Switzerland, What proportion of PWH who died fr
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): For individuals in Switzerland who had their first
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=2, iteration=1) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_2.1_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Which five antiretroviral drugs were most frequent
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Which five antiretroviral drugs were most frequent
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Is there a difference in the overall birth rate in
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Is there a difference in the overall birth rate in
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is the most common source of HIV infection in
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): In Switzerland, based on self-reported treatment a
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): From 2020, what is the proportion of PLWH in Switz
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): From 2020, what proportion of PWH in Switzerland c
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): In Switzerland, What proportion of PWH who died fr
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): For individuals in Switzerland who had their first
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=2, iteration=2) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_2.2_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Which five antiretroviral drugs were most frequent
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Which five antiretroviral drugs were most frequent
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Is there a difference in the overall birth rate in
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Is there a difference in the overall birth rate in
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is the most common source of HIV infection in
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): In Switzerland, based on self-reported treatment a
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): From 2020, what is the proportion of PLWH in Switz
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): From 2020, what proportion of PWH in Switzerland c
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): In Switzerland, What proportion of PWH who died fr
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): For individuals in Switzerland who had their first
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=2, iteration=3) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_2.3_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 58-year-old woman with HIV infection is brought 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 25-year-old sexually active male presents to an 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A mother with HIV has given birth to a healthy boy
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 35-year-old woman presents to a physician’s offi
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A physician scientist is looking for a more effici
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 50-year-old HIV-positive male presents to the ER
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 43-year-old man with HIV infection comes to the 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 28-year-old G1P0 woman at 16 weeks estimated ges
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 32-year-old man comes to the office for a routin
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 41-year-old HIV-positive male presents to the ER
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama):  What would be the best methodological approach to
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is known about DNA methylation profiles in pa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=3, iteration=1) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_3.1_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 58-year-old woman with HIV infection is brought 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 25-year-old sexually active male presents to an 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A mother with HIV has given birth to a healthy boy
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 35-year-old woman presents to a physician’s offi
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A physician scientist is looking for a more effici
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 50-year-old HIV-positive male presents to the ER
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 43-year-old man with HIV infection comes to the 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 28-year-old G1P0 woman at 16 weeks estimated ges
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 32-year-old man comes to the office for a routin
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 41-year-old HIV-positive male presents to the ER
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama):  What would be the best methodological approach to
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is known about DNA methylation profiles in pa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=3, iteration=2) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_3.2_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 58-year-old woman with HIV infection is brought 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 25-year-old sexually active male presents to an 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A mother with HIV has given birth to a healthy boy
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 35-year-old woman presents to a physician’s offi
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A physician scientist is looking for a more effici
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 50-year-old HIV-positive male presents to the ER
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 43-year-old man with HIV infection comes to the 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 28-year-old G1P0 woman at 16 weeks estimated ges
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 32-year-old man comes to the office for a routin
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 41-year-old HIV-positive male presents to the ER
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama):  What would be the best methodological approach to
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): What is known about DNA methylation profiles in pa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=3, iteration=3) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_3.3_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 26-year-old female with AIDS (CD4 count: 47) pre
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): You are reviewing raw data from a research study p
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 46-year-old Caucasian male with past medical his
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 44-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 25-year-old nulliparous woman at 8 weeks' gestat
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 33-year-old HIV-positive male is seen in clinic 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 30-year-old woman with HIV comes to the emergenc
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 35-year-old man comes to the physician because o
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 52-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 2300-g (5-lb 1-oz) male newborn is delivered to 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=4, iteration=1) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_4.1_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 26-year-old female with AIDS (CD4 count: 47) pre
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): You are reviewing raw data from a research study p
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 46-year-old Caucasian male with past medical his
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 44-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 25-year-old nulliparous woman at 8 weeks' gestat
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 33-year-old HIV-positive male is seen in clinic 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 30-year-old woman with HIV comes to the emergenc
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 35-year-old man comes to the physician because o
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 52-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 2300-g (5-lb 1-oz) male newborn is delivered to 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=4, iteration=2) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_4.2_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 26-year-old female with AIDS (CD4 count: 47) pre
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): You are reviewing raw data from a research study p
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 46-year-old Caucasian male with past medical his
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 44-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 25-year-old nulliparous woman at 8 weeks' gestat
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 33-year-old HIV-positive male is seen in clinic 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 30-year-old woman with HIV comes to the emergenc
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 35-year-old man comes to the physician because o
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 52-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 2300-g (5-lb 1-oz) male newborn is delivered to 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=4, iteration=3) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_4.3_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 54-year-old woman and a 54-year-old man with CVD
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): It is late at night in the clinic and I still have
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): I have two HIV-positive patients under ART. One is
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Do black people have higher HIV infection rates be
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): In Uganda, homosexuality is considered a felony th
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): 32-year-old male patient, MSM, no significant past
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 34-year-old HIV-positive pregnant woman presents
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 45-year-old woman comes to the clinic after test
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 27-year-old male who was diagnosed HIV positive 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 50-year-old female patient who recently started 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 32-year-old male patient on a strict PrEP regime
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=5, iteration=1) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_5.1_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 54-year-old woman and a 54-year-old man with CVD
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): It is late at night in the clinic and I still have
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): I have two HIV-positive patients under ART. One is
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Do black people have higher HIV infection rates be
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): In Uganda, homosexuality is considered a felony th
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): 32-year-old male patient, MSM, no significant past
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 34-year-old HIV-positive pregnant woman presents
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 45-year-old woman comes to the clinic after test
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 27-year-old male who was diagnosed HIV positive 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 50-year-old female patient who recently started 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 32-year-old male patient on a strict PrEP regime
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=5, iteration=2) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_5.2_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 54-year-old woman and a 54-year-old man with CVD
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): It is late at night in the clinic and I still have
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): I have two HIV-positive patients under ART. One is
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): Do black people have higher HIV infection rates be
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): In Uganda, homosexuality is considered a felony th
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): 32-year-old male patient, MSM, no significant past
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 34-year-old HIV-positive pregnant woman presents
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 45-year-old woman comes to the clinic after test
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 27-year-old male who was diagnosed HIV positive 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 50-year-old female patient who recently started 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 32-year-old male patient on a strict PrEP regime
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=5, iteration=3) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_5.3_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 26-year-old female with AIDS (CD4 count: 47) pre
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): You are reviewing raw data from a research study p
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 46-year-old Caucasian male with past medical his
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 44-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 25-year-old nulliparous woman at 8 weeks' gestat
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 33-year-old HIV-positive male is seen in clinic 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 30-year-old woman with HIV comes to the emergenc
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 35-year-old man comes to the physician because o
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 52-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 2300-g (5-lb 1-oz) male newborn is delivered to 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=6, iteration=1) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_6.1_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 26-year-old female with AIDS (CD4 count: 47) pre
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): You are reviewing raw data from a research study p
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 46-year-old Caucasian male with past medical his
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 44-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 25-year-old nulliparous woman at 8 weeks' gestat
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 33-year-old HIV-positive male is seen in clinic 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 30-year-old woman with HIV comes to the emergenc
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 35-year-old man comes to the physician because o
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 52-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 2300-g (5-lb 1-oz) male newborn is delivered to 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=6, iteration=2) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_6.2_HIV_EQ.json
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 26-year-old female with AIDS (CD4 count: 47) pre
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): You are reviewing raw data from a research study p
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 46-year-old Caucasian male with past medical his
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 44-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 25-year-old nulliparous woman at 8 weeks' gestat
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 33-year-old HIV-positive male is seen in clinic 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 30-year-old woman with HIV comes to the emergenc
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 35-year-old man comes to the physician because o
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 52-year-old man is brought to the emergency depa
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
API call for Llama failed with error: Connection error.
Starting inference for question locally (Llama): A 2300-g (5-lb 1-oz) male newborn is delivered to 
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Saved Llama answers (category=6, iteration=3) to: /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/raw/Llama_local/Llama_answers_category_6.3_HIV_EQ.json
Model inferences have been completed and saved to the output directory.
Saved prompts for Llama (category=1, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_1.1_HIV_EQ.json
Saved prompts for Llama (category=1, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_1.2_HIV_EQ.json
Saved prompts for Llama (category=1, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_1.3_HIV_EQ.json
Saved prompts for Llama (category=2, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_2.1_HIV_EQ.json
Saved prompts for Llama (category=2, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_2.2_HIV_EQ.json
Saved prompts for Llama (category=2, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_2.3_HIV_EQ.json
Saved prompts for Llama (category=3, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_3.1_HIV_EQ.json
Saved prompts for Llama (category=3, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_3.2_HIV_EQ.json
Saved prompts for Llama (category=3, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_3.3_HIV_EQ.json
Saved prompts for Llama (category=4, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_4.1_HIV_EQ.json
Saved prompts for Llama (category=4, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_4.2_HIV_EQ.json
Saved prompts for Llama (category=4, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_4.3_HIV_EQ.json
Saved prompts for Llama (category=5, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_5.1_HIV_EQ.json
Saved prompts for Llama (category=5, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_5.2_HIV_EQ.json
Saved prompts for Llama (category=5, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_5.3_HIV_EQ.json
Saved prompts for Llama (category=6, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_6.1_HIV_EQ.json
Saved prompts for Llama (category=6, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_6.2_HIV_EQ.json
Saved prompts for Llama (category=6, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_api/prompted_Llama_answers_category_6.3_HIV_EQ.json
Saved prompts for Llama (category=1, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_1.1_HIV_EQ.json
Saved prompts for Llama (category=1, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_1.2_HIV_EQ.json
Saved prompts for Llama (category=1, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_1.3_HIV_EQ.json
Saved prompts for Llama (category=2, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_2.1_HIV_EQ.json
Saved prompts for Llama (category=2, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_2.2_HIV_EQ.json
Saved prompts for Llama (category=2, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_2.3_HIV_EQ.json
Saved prompts for Llama (category=3, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_3.1_HIV_EQ.json
Saved prompts for Llama (category=3, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_3.2_HIV_EQ.json
Saved prompts for Llama (category=3, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_3.3_HIV_EQ.json
Saved prompts for Llama (category=4, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_4.1_HIV_EQ.json
Saved prompts for Llama (category=4, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_4.2_HIV_EQ.json
Saved prompts for Llama (category=4, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_4.3_HIV_EQ.json
Saved prompts for Llama (category=5, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_5.1_HIV_EQ.json
Saved prompts for Llama (category=5, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_5.2_HIV_EQ.json
Saved prompts for Llama (category=5, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_5.3_HIV_EQ.json
Saved prompts for Llama (category=6, iteration=1) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_6.1_HIV_EQ.json
Saved prompts for Llama (category=6, iteration=2) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_6.2_HIV_EQ.json
Saved prompts for Llama (category=6, iteration=3) -> /cluster/home/gcardenal/HIV/deploy_medical_LLM_evaluation/deploy_medical_llm_evaluation/model_answers/prompted_model_answers/Llama_local/prompted_Llama_answers_category_6.3_HIV_EQ.json
I0103 20:42:59.404000 22518965430080 torch/distributed/elastic/agent/server/api.py:885] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
I0103 20:42:59.404000 22518965430080 torch/distributed/elastic/agent/server/api.py:930] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish
I0103 20:42:59.404000 22518965430080 torch/distributed/elastic/agent/server/api.py:944] Done waiting for other agents. Elapsed: 0.0003490447998046875 seconds
