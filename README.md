# üè• HIVMedQA: Benchmarking large language models for HIV medical decision support

Evaluating the current state of LLMs for HIV management, examining their strengths and limitations.

## üéØ Table of Contents
- [Overview](#overview)
- [Getting Started](#getting-started)
- [Architecture & Workflow](#architecture--workflow)
- [Usage](#usage)
  - [Generate Model Answers](#get-model-answers)
  - [Obtain Evaluation Metrics (MedGPT & F1 score)](#obtain-metrics)
- [Contributing](#contributing)
- [License](#license)

## Overview

This study aims to evaluate the performance of current LLMs in the context of curbside consults for HIV care and provide actionable insights for their future development Specifically, we focus on:
- (1) assessing the reliability of LLMs as judges 
- (2) identifying the most effective lexical matching techniques for open-ended question evaluation
- (3) comparing the performance of small-scale versus large-scale LLMs
- (4) evaluating domain-specific (medical) models against generalized LLMs
- (5) benchmarking clinical skills of LLMs across the key dimensions comprehension, reasoning, knowledge recall, bias, and harm.

## Getting Started

## Architecture & Workflow

## Usage

##Contributing 
Contributions are welcome!
Please open an issue or pull request. Include tests and follow best practices when extending functionality.

##License 
