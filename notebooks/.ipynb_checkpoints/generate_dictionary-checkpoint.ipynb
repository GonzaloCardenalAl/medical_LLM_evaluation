{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5ba12f-9847-4c0e-b7df-8fd8fe198726",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      8\u001b[0m stanza\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmimic\u001b[39m\u001b[38;5;124m'\u001b[39m, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenize,pos,lemma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stanza'"
     ]
    }
   ],
   "source": [
    "#This notebook, generates a dictionary with synonyms from the true answers\n",
    "#Due to library conflicts, the code can not be run in a single conda environment. Therefore, the code is divided in two chunks to be run in separate environments.\n",
    "import os\n",
    "import json\n",
    "import stanza\n",
    "import spacy\n",
    "\n",
    "stanza.download('en', package='mimic', processors='tokenize,pos,lemma')\n",
    "biomed_nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma', package='mimic')\n",
    "\n",
    "\n",
    "def lemmatize_entities(entities):\n",
    "    \"\"\"\n",
    "    Lemmatize a set of entity strings using Stanza's en_biomedical pipeline.\n",
    "    \n",
    "    Args:\n",
    "        entities (set): A set of entity strings to be lemmatized.\n",
    "    Returns:\n",
    "        set: A set of lemmatized entity strings.\n",
    "    \"\"\"\n",
    "    lemmatized_entities = set()\n",
    "    for entity in entities:\n",
    "        doc = biomed_nlp(entity)\n",
    "        lemmatized_entity = \" \".join([word.lemma for sentence in doc.sentences for word in sentence.words])\n",
    "        lemmatized_entities.add(lemmatized_entity)\n",
    "    return lemmatized_entities\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace newline characters with a space and remove asterisks\n",
    "    text = text.replace('\\n', ' ').replace('*', '')\n",
    "    # Remove any extra spaces that may have resulted\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "category_ids = [str(num) for num in range(1, 6)] \n",
    "\n",
    "all_entities_true = set()\n",
    "\n",
    "for category_id in category_ids:\n",
    "    file_path = f\".././deploy_medical_llm_evaluation/questions_files/HIV_evaluation_questionare_category_{category_id}.json\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    nlp = spacy.load('en_core_sci_lg')\n",
    "    \n",
    "    # Process each answer pair\n",
    "    for idx, item in enumerate(data):\n",
    "        # Load the response field as JSON\n",
    "        try:\n",
    "            answer = item.get('true_answer', '')\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error for file {answer_file_name}, question {idx}: {e}\")\n",
    "            continue\n",
    "            \n",
    "       # answer = response_json.get('true_answer', '')\n",
    "        answer_clean = clean_text(answer)\n",
    "        doc_answer = nlp(answer_clean)\n",
    "        \n",
    "        # Proceed with entity extraction\n",
    "        entities_true = set(ent.text.lower() for ent in doc_answer.ents)\n",
    "\n",
    "        entities_true_lemmatized = lemmatize_entities(entities_true)\n",
    "        all_entities_true.update(entities_true_lemmatized)\n",
    "\n",
    "entities_list = sorted(all_entities_true)\n",
    "output_file_path = \"./extracted_entities.txt\"\n",
    "\n",
    "\n",
    "# Save the entities list to the file\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for entity in entities_list:\n",
    "        file.write(f\"'{entity}', \")\n",
    "\n",
    "print(f\"Entities list has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b93a1-d63b-4f09-b7f8-2f18f03ca505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "file_path_txt = \"./extracted_entities.txt\"\n",
    "\n",
    "with open(file_path_txt, \"r\") as file:\n",
    "    raw_entities_list = file.read().strip().split(\", \")\n",
    "\n",
    "cleaned_list = [item.strip(\"'\") for item in raw_entities_list]\n",
    "\n",
    "gpt4_api_key = \"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\"\n",
    "gpt4_base_url = \"http://148.187.108.173:8080\"\n",
    "\n",
    "client = OpenAI(api_key=gpt4_api_key)\n",
    "\n",
    "batch_size = 10\n",
    "batches = [cleaned_list[i:i + batch_size] for i in range(0, len(cleaned_list), batch_size)]\n",
    "\n",
    "synonym_prompt = \"You are working as a synonym dictionary for precise medical terms. For medical terms, especially medications, synonyms should only include chemical names, brand names, or closely related alternative scientific terms—avoid broader pharmacological categories. For each input term, provide a list of synonyms. Your answer should be provided in the following format (give maximum a set of 5 synonyms and don’t generate any other information):\\n{\\n    \\\"term_1\\\": [\\\"synonym_1\\\", \\\"synonym_2\\\", \\\"synonym_3\\\"],\\n    \\\"term_2\\\": [\\\"synonym_1\\\", \\\"synonym_2\\\", \\\"synonym_3\\\", \\\"synonym_4\\\", \\\"synonym_5\\\"]\\n}\"\n",
    "\n",
    "final_synonyms_dict = {}\n",
    "\n",
    "for batch in batches:\n",
    "    input_terms = \", \".join(f\"'{term}'\" for term in batch) \n",
    "    formatted_input_terms = f\"{{{input_terms}}}\"  \n",
    "    try:\n",
    "        res = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": synonym_prompt},\n",
    "                {\"role\": \"user\", \"content\": input_terms}\n",
    "            ],\n",
    "            stream=False,\n",
    "            temperature=0,\n",
    "        )\n",
    "        assistant_reply = res.choices[0].message.content.strip()\n",
    "        \n",
    "        # Parse the GPT response as JSON\n",
    "        synonyms_dict = json.loads(assistant_reply)\n",
    "        \n",
    "        # Update the final dictionary\n",
    "        final_synonyms_dict.update(synonyms_dict)\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding GPT response: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating synonyms for batch {batch}: {e}\")\n",
    "\n",
    "# Save the synonyms dictionary to a JSON file\n",
    "output_file = \"synonyms_dictionary.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(final_synonyms_dict, f, indent=4)\n",
    "\n",
    "print(f\"Synonyms dictionary saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
